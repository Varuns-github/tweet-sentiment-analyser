{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMZ-25w_PhUH",
        "outputId": "677f0517-9922-4f58-cdde-10c74e854047"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7gLum9iPl_3",
        "outputId": "845e27e0-b88f-4631-f6e7-05c3ba2a7b2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "                                          clean_text  Negative  Neutral  \\\n",
            "0  TheSocialDilemma is an eye opener isn t it ple...         0        1   \n",
            "1  TheSocialDilemma If we don t agree on what is ...         0        0   \n",
            "2  Watching TheSocialDilemma scary to see social ...         1        0   \n",
            "3  You check your social media before you pee in ...         0        0   \n",
            "4  watch thesocialdilemma and see what s actually...         1        0   \n",
            "\n",
            "   Positive  \n",
            "0         0  \n",
            "1         1  \n",
            "2         0  \n",
            "3         1  \n",
            "4         0  \n",
            "10386\n",
            "Epoch 1/10\n",
            "93/93 [==============================] - 7s 59ms/step - loss: 0.5455 - accuracy: 0.5789\n",
            "Epoch 2/10\n",
            "93/93 [==============================] - 6s 68ms/step - loss: 0.3248 - accuracy: 0.7943\n",
            "Epoch 3/10\n",
            "93/93 [==============================] - 5s 58ms/step - loss: 0.1576 - accuracy: 0.9171\n",
            "Epoch 3: early stopping\n",
            "160/160 [==============================] - 2s 9ms/step - loss: 0.2834 - accuracy: 0.8371\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([2, 2, 2, ..., 0, 1, 1])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "from string import punctuation\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/*path_to_dataset*/datasets/tweets_train.csv', keep_default_na=False)\n",
        "df = df[['clean_text','Sentiment']]\n",
        "\n",
        "df.head()\n",
        "\n",
        "one_hot = pd.get_dummies(df[\"Sentiment\"])\n",
        "df.drop(['Sentiment'],axis=1,inplace=True)\n",
        "df = pd.concat([df,one_hot],axis=1)\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "def get_text_processing(text):\n",
        "    stpword = stopwords.words('english')\n",
        "    no_punctuation = [char for char in text if char not in punctuation]\n",
        "    no_punctuation = ''.join(no_punctuation)\n",
        "    return ' '.join([word for word in no_punctuation.split() if word.lower() not in stpword])\n",
        "\n",
        "df['clean_text'] = df['clean_text'].apply(get_text_processing)\n",
        "df.head()\n",
        "\n",
        "\n",
        "X = df['clean_text'].values\n",
        "y = df.drop('clean_text', axis=1).values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index=tokenizer.word_index\n",
        "vocab_size = len(word_index)+1\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "maxlen = 100\n",
        "X_train_pad = pad_sequences(X_train, padding='pre', maxlen=maxlen)\n",
        "X_test_pad = pad_sequences(X_test, padding='pre', maxlen=maxlen)\n",
        "\n",
        "# create rnn model multilabel tweet classification\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(len(X_train_pad), 64),\n",
        "    tf.keras.layers.SimpleRNN(64),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "early_stop = EarlyStopping(monitor='accuracy', mode='min', verbose=1, patience=2)\n",
        "\n",
        "model.fit(X_train_pad, y_train, epochs=10, batch_size=128, callbacks=early_stop)\n",
        "\n",
        "model.evaluate(X_test_pad, y_test,)\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/*path_to_dataset*/datasets/tweets_test.csv', keep_default_na=False)\n",
        "df = df[['clean_text']]\n",
        "\n",
        "df['clean_text'] = df['clean_text'].apply(get_text_processing)\n",
        "\n",
        "X_pred = df['clean_text'].values\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_pred)\n",
        "word_index=tokenizer.word_index\n",
        "vocab_size = len(word_index)+1\n",
        "\n",
        "X_pred = tokenizer.texts_to_sequences(X_pred)\n",
        "\n",
        "maxlen = 100\n",
        "X_pred = pad_sequences(X_pred, padding='pre', maxlen=maxlen)\n",
        "\n",
        "y_prob = model.predict(X_pred)\n",
        "\n",
        "y_classes = y_prob.argmax(axis=-1)\n",
        "\n",
        "y_classes"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "tweet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
